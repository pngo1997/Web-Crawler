{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201fe1b5-206a-4947-9f80-57633940be50",
   "metadata": {},
   "source": [
    "https://www.cdm.depaul.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d48b5f6-28b2-4b0b-a1d4-bb51bc6e773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen, urljoin, urlparse, Request\n",
    "from html.parser import HTMLParser\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import threading\n",
    "from queue import Queue\n",
    "#Allows to create directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67156b36-b7be-44f9-a789-d82ce5e8fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProject (directory):\n",
    "    '''Create folder to store files keep track of each web page.'''\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "    #If the path is not exits. \n",
    "        os.makedirs(directory)\n",
    "        #Create directory recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55ce39c-b57e-474e-ad0d-d5e919ad3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file (linkPath, data):\n",
    "    '''Write links to outfiles.'''\n",
    "    \n",
    "    outfile = open(linkPath, 'w')\n",
    "    outfile.write(data)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd29111-eb2c-4f35-938c-e3e49d8f990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFiles(projectName, baseURL):\n",
    "    '''Create files within folders above.'''\n",
    "    \n",
    "    waitLinks = projectName + '/waitLink.txt'\n",
    "    #Store a list of url links waiting to be crawled. Constantly updated. \n",
    "    crawledLinks = projectName + '/crawledLink.txt'\n",
    "    #Store a list of url links were crawled. Constantly updated. \n",
    "    \n",
    "    #After a web page is crawled, the link will move from waitLinks to crawledLinks. \n",
    "    if not os.path.isfile(waitLinks):\n",
    "    #If the file does not exists.\n",
    "        write_file(waitLinks, baseURL)\n",
    "        #Create a new text file, using baseURL as initial page (starting point).\n",
    "        \n",
    "    if not os.path.isfile(crawledLinks):\n",
    "        write_file(crawledLinks, '')\n",
    "        #Initially will be empty, because it hasn't crawled any website yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7429bf5c-61c0-43f8-a154-89a92b8e5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendFiles(linkPath, data):\n",
    "    '''Adding data to existing file.'''\n",
    "    \n",
    "    with open (linkPath, 'a') as infile: \n",
    "        infile.write(data + '\\n')\n",
    "        #Adding new links to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a854080-0cc2-4aa7-8144-7052b4c3e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteFiles_link(linkPath):\n",
    "    '''Delete content in a file.'''\n",
    "    \n",
    "    with open (linkPath, 'w'): pass\n",
    "    #Open file 'w' will wipe all current content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73670839-a98e-4f20-b18e-0b934ba524ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_Set(fileName):\n",
    "    '''Read a file and convert each link to a set() item.'''\n",
    "    \n",
    "    ans = set()\n",
    "    with open (fileName, 'rt') as infile:\n",
    "    #Open and read file the file as text. \n",
    "        for link in infile:\n",
    "            ans.add(link.replace('\\n', ''))\n",
    "            #Delete the new line character added in def appendFiles().\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43edc86d-5b34-4cda-bb90-6b52835e3285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkset_Files(links, file):\n",
    "    '''Iterate through a set, each link will be converted to a new line in a file.'''\n",
    "    #Links set() is obtained through collector(HTMLparser)\n",
    "    \n",
    "    deleteFiles_link(file)\n",
    "    #Wipe all current content because link set() has updated url links. \n",
    "    for link in links:\n",
    "        appendFiles(file, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0c960c-55ae-4465-a7c4-70ffb99cd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(HTMLParser):\n",
    "    \"\"\"Collects hyperlink URLs into a list\"\"\"\n",
    "\n",
    "    ignore_tags = ['script', 'noscript', 'input', 'meta', 'title', 'style', 'form']\n",
    "    tag = ''\n",
    "    def __init__(self, base_url):\n",
    "        \"\"\" initializes parser, the url, and a list \"\"\"\n",
    "        HTMLParser.__init__(self)\n",
    "        self.base_url = base_url\n",
    "        #self.page_url = page_url\n",
    "        self.links = set()\n",
    "        self.trash = []\n",
    "        self.data = []\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        \"\"\" collects hyperlink URLs in their absolute format \"\"\"      \n",
    "        self.tag = tag\n",
    "        #Assign tag to an instance variable.\n",
    "        if self.tag == 'a':\n",
    "        #<a> tag defines a hyperlink\n",
    "        \n",
    "            for (attribute,value) in attrs:\n",
    "            #Iterate over each piece of HTML. \n",
    "                if attribute == 'href':\n",
    "                #Specifies a URL.  \n",
    "                \n",
    "                    absolute = urljoin(self.base_url, value.strip().lower())\n",
    "                    #Using strip() because ' https://depaulmagazine.com/2022/12/08/professional-partners/' has leading whitespace \n",
    "                    #Assign absolute variable: Construct absolute URL, join base URL and value.\n",
    "                    \n",
    "                    if absolute[:4] == 'http':\n",
    "                        self.links.add(absolute)\n",
    "                        if absolute.find('course-evaluation') == -1: \n",
    "                            self.trash.append(absolute)\n",
    "                    else: self.trash.append(absolute)\n",
    "                            \n",
    "                    \n",
    "    def handle_data(self, data):\n",
    "        if self.tag not in self.ignore_tags:\n",
    "            for word in data.strip().split():\n",
    "                gen_word = word.lower()\n",
    "                self.data.append(gen_word)\n",
    "\n",
    "    def getLinks(self):\n",
    "        \"\"\" returns hyperlinks URLs in their absolute format \"\"\"\n",
    "        return self.links\n",
    "\n",
    "    def getTrash(self):\n",
    "        \"\"\" returns hyperlinks URLs in their absolute format \"\"\"\n",
    "        return self.trash\n",
    "    \n",
    "    def getData(self):\n",
    "        \"\"\" returns the data (accumulated in the instance variable) \"\"\"\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65fbd56d-41bb-459c-86e5-c1adb79348a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class masterCrawl: \n",
    "    \n",
    "    projectName = ''\n",
    "    baseURL = ''\n",
    "    domainName = '' \n",
    "    #Making sure crawling within CDM website\n",
    "    \n",
    "    waitFile = ''\n",
    "    #waitFile to store wait set()\n",
    "    crawledFile = ''\n",
    "    #crawledFile to store crawled set()\n",
    "    \n",
    "    wait = set()\n",
    "    crawled = set()\n",
    "    \n",
    "       \n",
    "    def __init__ (self, projectName, baseURL, domainName):\n",
    "        masterCrawl.projectName = projectName\n",
    "        masterCrawl.baseURL = baseURL\n",
    "        masterCrawl.domainName = domainName\n",
    "        masterCrawl.waitFile = masterCrawl.projectName + '/waitLink.txt'\n",
    "        masterCrawl.crawledFile = masterCrawl.projectName + '/crawledLink.txt'\n",
    "        self.boot()\n",
    "        self.crawl('Crawl 1', masterCrawl.baseURL)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boot():\n",
    "        '''Asking first crawl to create directory and files.'''\n",
    "        createProject (masterCrawl.projectName)\n",
    "        createFiles(masterCrawl.projectName, masterCrawl.baseURL)\n",
    "        masterCrawl.wait = file_Set(masterCrawl.waitFile)\n",
    "        masterCrawl.crawled = file_Set(masterCrawl.crawledFile)\n",
    "        \n",
    "    @staticmethod\n",
    "    def crawl(crawl_num, pageURL):\n",
    "        ''' Recursive web crawler that calls analyze() on every visited web page.'''\n",
    "        if pageURL not in masterCrawl.crawled:\n",
    "            print(crawl_num + ':' + pageURL)\n",
    "            print ('Wait  ' + str(len(masterCrawl.wait)) + '  | Crawled  ' + str(len(masterCrawl.crawled)))\n",
    "            masterCrawl.addWaitlinks(masterCrawl.compileLinks(pageURL))\n",
    "            #Retrieve a list of respective hyperlink in pageURL. Then feed to waitFile.\n",
    "            masterCrawl.wait.remove(pageURL)\n",
    "            #Remove from wait file\n",
    "            masterCrawl.crawled.add(pageURL)\n",
    "            #Adding to crawled file\n",
    "            masterCrawl.updateFiles()\n",
    "            \n",
    "    @staticmethod\n",
    "    def compileLinks(pageURL):\n",
    "        '''Connect to the webpage, and convert HTML to stirng format. And returns all URLs within page.'''\n",
    "      \n",
    "        page_url =  'https://www.cdm.depaul.edu/'\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64)\"\n",
    "        request = Request(url)\n",
    "        request.add_header(\"User-Agent\",user_agent)\n",
    "        content = urlopen(request).read().decode()\n",
    "        collector = Collector(url)\n",
    "        collector.feed(content)\n",
    "        return collector.getLinks()\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def addWaitlinks(links):\n",
    "        '''Takes a set of link and add to existed waiting list.'''\n",
    "        for url in links:\n",
    "            if url in masterCrawl.wait:\n",
    "                continue\n",
    "            if url in masterCrawl.crawled:\n",
    "                continue \n",
    "            if masterCrawl.domainName not in url:\n",
    "                continue\n",
    "            masterCrawl.wait.add(url)\n",
    "            #Make sure crawl scope is within CDM \n",
    "            \n",
    "    @staticmethod\n",
    "    def updateFiles():\n",
    "        linkset_Files(masterCrawl.wait, masterCrawl.waitFile)\n",
    "        linkset_Files(masterCrawl.crawled, masterCrawl.crawledFile)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06faa624-8d23-4d49-b80e-789e9e9f5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_domainName(url):\n",
    "    '''Get the entire domain name.'''\n",
    "    try:\n",
    "        return urlparse(url).netloc\n",
    "    except: return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5592f814-a6fe-469b-be43-001a14a3bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domainName(url):\n",
    "    '''Get the main domain name. Which is the last two.'''\n",
    "    try:\n",
    "        result = sub_domainName(url).split('.')\n",
    "        return result [-3] + '.' + result [-2] + '.' + result[-1]\n",
    "    except: return ''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3565d9c-d805-461e-bae3-2fe725917bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://my.cdm.depaul.edu/v2',\n",
       " 'https://www.cdm.depaul.edu/#ctl00_pagebody',\n",
       " 'https://www.cdm.depaul.edu/about/pages/about.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/alert.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/alumni/alumni.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/contactus.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/diversity-equity-inclusion.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/jobs.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/partnerships-and-outreach.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10273',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10315',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10345',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10346',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10356',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10360',\n",
       " 'https://www.cdm.depaul.edu/about/pages/rsvp.aspx?evid=10365',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/academics.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/bachelors-programs.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/bfa-in-graphic-design.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/certificates.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/combineddegrees.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/coursecatalog.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/courseschedule.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/coursesyllabisearch.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/current/undergraduateminors.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/graduateprograms.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/high-school-programs.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/majors-and-degrees.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/school-of-cinematic-arts.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/school-of-computing.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/school-of-design.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/pages/study-abroad.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/research/pages/grants.aspx',\n",
       " 'https://www.cdm.depaul.edu/academics/research/pages/studentresearch.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/graduateadmissions.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/internationaladmissions.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/nondegreeoptions.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/prospective-students.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/requestinformation.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/studentambassadors.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/tuitionandfinancialaid.aspx',\n",
       " 'https://www.cdm.depaul.edu/admission-and-aid/pages/undergraduateadmissions.aspx',\n",
       " 'https://www.cdm.depaul.edu/faculty-and-staff/pages/administration.aspx',\n",
       " 'https://www.cdm.depaul.edu/faculty-and-staff/pages/faculty-and-staff.aspx',\n",
       " 'https://www.cdm.depaul.edu/faculty-and-staff/pages/faculty.aspx',\n",
       " 'https://www.cdm.depaul.edu/faculty-and-staff/pages/researchandcreativeactivities.aspx',\n",
       " 'https://www.cdm.depaul.edu/ipd/programs/pages/certificate-programs.aspx',\n",
       " 'https://www.cdm.depaul.edu/onlinelearning/pages/default.aspx',\n",
       " 'https://www.cdm.depaul.edu/pages/default2.aspx',\n",
       " 'https://www.cdm.depaul.edu/pages/privacy.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/cdm-tuition-funding.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/current-students.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/faq.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/jarvis-student-center-for-innovation-and-collaboration.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/jobs-splash.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/labsandresources/labs-and-software.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/meetourstaff.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/organizations.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/policiesandprocedures.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/production-resources.aspx',\n",
       " 'https://www.cdm.depaul.edu/student-resources/pages/student-tutoring.aspx'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectName = 'SurfCDM'\n",
    "MainURL = 'http://www..cdm.depaul.edu/'\n",
    "domainName = get_domainName(MainURL)\n",
    "waitFile = projectName + '/waitLink.txt'\n",
    "crawledFile = projectName + '/crawledLink.txt'\n",
    "crawl_num = 8\n",
    "\n",
    "queue = Queue()\n",
    "#masterCrawl(projectName, MainURL, domainName)\n",
    "masterCrawl.addWaitlinks(masterCrawl.compileLinks(MainURL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a09f4c6-004b-41a0-b431-a7d2c3613417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawlWork():\n",
    "    wait_link = file_Set(waitFile)\n",
    "    if len(waitFile) > 0:\n",
    "        print(str(len(waitFile)) + ' waiting links')\n",
    "        createJobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3c85396-a86e-4f58-9084-3e2d34ac3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJobs():\n",
    "    for link in file_Set(waitFile):\n",
    "        queue.put(link)\n",
    "    queue.join()\n",
    "    crawlWork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d2a7093-7ef7-478a-9693-26875b7e2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startCrawl():\n",
    "    for times in range(crawl_num):\n",
    "    #Create 8 crawl engines. \n",
    "        crawl = threading.Thread(target = work)\n",
    "        crawl.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0ade99c-65df-4ba7-ad77-27aac171e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def work():\n",
    "    while True:\n",
    "        url = queue.get()\n",
    "        masterCrawl.crawl(threading.current_thread().name, url)\n",
    "        queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc5700-8586-47e6-802e-fcd611a3ff38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
